# Attention NLP Playground

Educational exploration of an NLP attention layer, from embeddings to a final encoder head. 
This Jupyter notebook demonstrates the key parts of a mini-Transformer step by step.

## Objective

The goal of this project is to understand how token, ``positional embeddings``, ``multi-head attention``, ``feed-forward layers``, ``skip connections``, ``normalization layers``, 
and the encoder head works together in NLP models.

## Repository Contents

- `Attention_Layer_Exploration.ipynb` – main Jupyter notebook with explanations, visualizations, and code.
- Optional: `images/` – diagrams or illustrations of the architecture.

## Getting Started

1. Clone the repository:
   ```bash
   git clone https://github.com/Ryan2486/nlp-attention-playground.git
   ```
2. Open the notebook in Jupyter Lab or Jupyter Notebook environment. 
3. Run the cells sequentially to explore each component and understand the data flow.

## EDUCATIONAL PURPOSES

This project is intended purely for educational purposes to help learners visualize and understand the mechanics of attention layers in NLP. 
It can serve as a hands-on guide for students or anyone curious about the inner workings of Transformers.